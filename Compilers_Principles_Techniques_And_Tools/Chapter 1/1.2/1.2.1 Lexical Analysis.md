The first phase of the compiler is called lexical Analysis or scanning.

The lexical analyzer reads a stream of characters making up the source program, and groups the characters into meaningful sequences called lexemes.

For each lexemes, the lexical analyzer produces a token of the form:
<token-name,attribute-value>

These tokens get passed to the syntax analysis phase.
In the token, the token-name is an abstract symbol that is used during syntax analysis and the second component attribute-value points to an entry in the symbol table for this token.

Entries in the symbol table is needed for semantic analysis and code generation.

Take for example:

position = initial + rate \*60

The lexemes that this might produce could be: 

\<id, 1\>  (id is an abstract symbol representing an identifier and 1 points to the symbol table entry for  position). The symbol table entry for an identifier holds information about the identifier such as it's name and it's type.

\<=\> this lexeme doesn't require an attribute-value (although assign could've been used for an abstract identifier).

\<id, 2\> initial is a lexeme that is mapped into this token where 2 points to the entry in the symbol table for it.

\+ is a lexeme that's mapped to the token \<+\>

the rest of the example is pretty much self-explanatory from here. The blanks are discarded by the lexical analyzer. The final form for all of the tokens:

\<id,1\> \<=\> \<id,2\> \<\+\> \<id,3\> \<\*\> \<60\>

technically, the 60 could be turned into a token like \<number,4\> where 4 points to the symbol table for the internal representation of integer 60, but this discussion will happen more in chapter 2.

